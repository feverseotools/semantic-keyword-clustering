try:
    import spacy
    try:
        nlp = spacy.load("en_core_web_sm")
        spacy_available = True
    except:
        # Si el modelo no est√° descargado, intentar descargarlo usando la API de spaCy
        try:
            import spacy.cli
            spacy.cli.download("en_core_web_sm")
            nlp = spacy.load("en_core_web_sm")
            spacy_available = True
        except:
            st.warning("No se pudo descargar el modelo de spaCy. Se usar√°n funcionalidades limitadas.")
            spacy_available = False
except ImportError:
    spacy_available = False

try:
    import hdbscan
    hdbscan_available = True
except ImportError:
    hdbscan_available = False

# Descargar recursos de NLTK al inicio para evitar problemas posteriores
try:
    import nltk
    for resource in ['stopwords', 'punkt', 'wordnet']:
        try:
            nltk.data.find(f'tokenizers/{resource}')
        except LookupError:
            nltk.download(resource, quiet=True)
except Exception as e:
    pass  # Continuar incluso si la descarga falla
# Funci√≥n para calcular el coste estimado de la API
def calculate_api_cost(num_keywords, selected_model="gpt-3.5-turbo", num_clusters=10):
    """
    Calcula el coste estimado de usar la API de OpenAI basado en el n√∫mero de keywords
    
    Args:
        num_keywords: N√∫mero total de keywords
        selected_model: Modelo para nombrar clusters (gpt-3.5-turbo o gpt-4)
        num_clusters: N√∫mero estimado de clusters
    
    Returns:
        dict: Desglose de costes por componente y coste total
    """
    # Precios actualizados (marzo 2025)
    EMBEDDING_COST_PER_1K = 0.02  # text-embedding-3-small por 1K tokens
    
    # Costes de GPT-3.5-Turbo
    GPT35_INPUT_COST_PER_1K = 0.0005
    GPT35_OUTPUT_COST_PER_1K = 0.0015
    
    # Costes de GPT-4
    GPT4_INPUT_COST_PER_1K = 0.03
    GPT4_OUTPUT_COST_PER_1K = 0.06
    
    # Resultados
    results = {
        "embedding_cost": 0,
        "naming_cost": 0,
        "total_cost": 0,
        "processed_keywords": 0
    }
    
    # 1. Coste de embeddings - limitado a 5000 keywords
    keywords_for_embeddings = min(num_keywords, 5000)
    results["processed_keywords"] = keywords_for_embeddings
    
    # Estimamos un promedio de 2 tokens por keyword (algunas tendr√°n 1, otras m√°s)
    estimated_tokens = keywords_for_embeddings * 2
    results["embedding_cost"] = (estimated_tokens / 1000) * EMBEDDING_COST_PER_1K
    
    # 2. Coste de nombrar clusters
    # Estimamos tokens para nombrar clusters (depende del n√∫mero de clusters)
    # El prompt para an√°lisis + Keywords representativas (aprox. 15 por cluster) + Respuesta
    avg_tokens_per_cluster = 200  # Tokens por cluster para el input (incluyendo keywords)
    avg_output_tokens_per_cluster = 80  # Tokens de salida por cluster (nombre + descripci√≥n en JSON)
    
    estimated_input_tokens = num_clusters * avg_tokens_per_cluster
    estimated_output_tokens = num_clusters * avg_output_tokens_per_cluster
    
    if selected_model == "gpt-3.5-turbo":
        input_cost = (estimated_input_tokens / 1000) * GPT35_INPUT_COST_PER_1K
        output_cost = (estimated_output_tokens / 1000) * GPT35_OUTPUT_COST_PER_1K
    else:  # gpt-4
        input_cost = (estimated_input_tokens / 1000) * GPT4_INPUT_COST_PER_1K
        output_cost = (estimated_output_tokens / 1000) * GPT4_OUTPUT_COST_PER_1K
    
    results["naming_cost"] = input_cost + output_cost
    
    # 3. Coste total
    results["total_cost"] = results["embedding_cost"] + results["naming_cost"]
    
    return results
# Widget de calculadora de costes para Streamlit
def add_cost_calculator():
    st.sidebar.markdown("---")
    with st.sidebar.expander("üí∞ Calculadora de Costes API", expanded=False):
        st.markdown("""
        ### Calculadora de Costes de API
        
        Calcula el coste aproximado de procesar tus keywords con OpenAI.
        """)
        
        # Input de n√∫mero de keywords
        calc_num_keywords = st.number_input(
            "N√∫mero de keywords a procesar", 
            min_value=100, 
            max_value=100000, 
            value=1000,
            step=500
        )
        
        # Input de n√∫mero de clusters
        calc_num_clusters = st.number_input(
            "N√∫mero aproximado de clusters",
            min_value=2,
            max_value=50,
            value=10,
            step=1
        )
        
        # Selecci√≥n de modelo
        calc_model = st.radio(
            "Modelo para nombrar clusters",
            options=["gpt-3.5-turbo", "gpt-4"],
            index=0,
            horizontal=True
        )
        
        # Bot√≥n para calcular
        if st.button("Calcular Coste Estimado", use_container_width=True):
            cost_results = calculate_api_cost(calc_num_keywords, calc_model, calc_num_clusters)
            
            # Mostrar resultados
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric(
                    "Keywords procesadas con OpenAI", 
                    f"{cost_results['processed_keywords']:,}",
                    help="OpenAI procesa hasta 5,000 keywords, el resto se propaga mediante similitud"
                )
                
                st.metric(
                    "Coste embeddings", 
                    f"${cost_results['embedding_cost']:.4f}",
                    help="Coste de generar vectores sem√°nticos con text-embedding-3-small"
                )
            
            with col2:
                st.metric(
                    "Coste nombrar clusters", 
                    f"${cost_results['naming_cost']:.4f}",
                    help=f"Coste de generar nombres y descripciones usando {calc_model}"
                )
                
                st.metric(
                    "COSTE TOTAL", 
                    f"${cost_results['total_cost']:.4f}",
                    help="Coste total estimado de API (no incluye recursos computacionales)"
                )
            
            st.info("""
            **Nota:** Esta es una estimaci√≥n aproximada. El coste real puede variar 
            seg√∫n la longitud de las keywords y la complejidad de los clusters.
            Usando Sentence Transformers como alternativa, el coste es $0.
            """)

# Funci√≥n para mostrar coste estimado del CSV cargado
def show_csv_cost_estimate(num_keywords, selected_model="gpt-3.5-turbo", num_clusters=10):
    if num_keywords > 0:
        cost_results = calculate_api_cost(num_keywords, selected_model, num_clusters)
        
        with st.sidebar.expander("üí∞ Coste Estimado (CSV actual)", expanded=True):
            st.markdown(f"### Coste Estimado para {num_keywords:,} Keywords")
            
            # Mostrar desglose
            st.markdown(f"""
            - **Keywords procesadas con OpenAI**: {cost_results['processed_keywords']:,}
            - **Coste embeddings**: ${cost_results['embedding_cost']:.4f}
            - **Coste nombrar clusters**: ${cost_results['naming_cost']:.4f}
            - **COSTE TOTAL**: ${cost_results['total_cost']:.4f}
            """)
            
            if cost_results['processed_keywords'] < num_keywords:
                st.info(f"""
                Se procesan directamente {cost_results['processed_keywords']:,} keywords con OpenAI.
                Las {num_keywords - cost_results['processed_keywords']:,} restantes se procesar√°n 
                mediante propagaci√≥n de similitud sem√°ntica.
                """)
            
            st.markdown("""
            **Ahorro de costes**: Si prefieres no usar OpenAI, puedes 
            utilizar SentenceTransformers como alternativa gratuita con 
            buenos resultados.
            """)
# MEJORA 4: Preprocesamiento Sem√°ntico Mejorado
def enhanced_preprocessing(text, use_lemmatization=True):
    """Preprocesamiento mejorado con tratamiento de entidades y n-gramas"""
    if not isinstance(text, str) or not text.strip():
        return ""
    
    try:
        # Usar spaCy para un an√°lisis ling√º√≠stico m√°s avanzado
        if spacy_available:
            doc = nlp(text.lower())
            
            # Conservar entidades nombradas completas
            entities = [ent.text for ent in doc.ents]
            
            # Extraer tokens relevantes (no stopwords)
            tokens = []
            for token in doc:
                if not token.is_stop and token.is_alpha and len(token.text) > 1:
                    tokens.append(token.lemma_)
            
            # Extraer bigramas relevantes
            bigrams = []
            for i in range(len(doc) - 1):
                if (not doc[i].is_stop and not doc[i+1].is_stop and 
                    doc[i].is_alpha and doc[i+1].is_alpha):
                    bigrams.append(f"{doc[i].lemma_}_{doc[i+1].lemma_}")
            
            # Combinar todo preservando las entidades
            processed_parts = tokens + bigrams + entities
            return " ".join(processed_parts)
        else:
            # Fallback al m√©todo original si spaCy no est√° disponible
            return preprocess_text(text, use_lemmatization)
    except Exception as e:
        return text.lower() if isinstance(text, str) else ""

# Funci√≥n original de preprocesamiento como fallback
def preprocess_text(text, use_lemmatization=True):
    if not isinstance(text, str) or not text.strip():
        return ""
    
    try:
        # Convertir a min√∫sculas
        text = text.lower()
        
        # Tokenizar
        tokens = word_tokenize(text)
        
        # Cargar stopwords
        try:
            stop_words = set(stopwords.words('english'))
        except:
            # Fallback b√°sico si no se pueden cargar stopwords
            stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what', 'in', 'on', 'to', 'for'}
        
        # Filtrar stopwords
        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]
        
        # Lematizaci√≥n (opcional)
        if use_lemmatization:
            try:
                lemmatizer = WordNetLemmatizer()
                tokens = [lemmatizer.lemmatize(t) for t in tokens]
            except Exception as e:
                pass  # Continuar sin lematizaci√≥n si falla
        
        return " ".join(tokens)
    except Exception as e:
        # Manejo de errores para garantizar que siempre se devuelva algo
        return text.lower() if isinstance(text, str) else ""

# Funci√≥n para preprocesar keywords
def preprocess_keywords(keywords, use_advanced=True):
    processed_keywords = []
    
    progress_bar = st.progress(0)
    total = len(keywords)
    
    for i, keyword in enumerate(keywords):
        if use_advanced and spacy_available:
            processed_keywords.append(enhanced_preprocessing(keyword))
        else:
            processed_keywords.append(preprocess_text(keyword))
        
        # Update progress bar every 100 items
        if i % 100 == 0:
            progress_bar.progress(min(i / total, 1.0))
    
    progress_bar.progress(1.0)
    return processed_keywords
# MEJORA 1: Embeddings mejorados con prioridad a OpenAI y l√≠mite de 5000 keywords
def generate_embeddings(df, openai_available, openai_api_key=None):
    st.info("Generando embeddings para las keywords...")
    
    # Opci√≥n 1: Usar OpenAI si est√° disponible y se proporciona API key
    if openai_available and openai_api_key:
        try:
            st.info("Usando embeddings de OpenAI (alta precisi√≥n sem√°ntica)")
            # Configurar OpenAI
            os.environ["OPENAI_API_KEY"] = openai_api_key
            client = OpenAI()
            
            # Procesar en batches para minimizar costos
            keywords = df['keyword_processed'].fillna('').tolist()
            all_embeddings = []
            
            # Aumentado a 5000 keywords (en lugar de 1000)
            if len(keywords) > 5000:
                st.warning(f"Limitando a 5000 keywords representativas de las {len(keywords)} totales")
                # Seleccionar keywords estrat√©gicamente (no solo las primeras)
                step = max(1, len(keywords) // 5000)
                sample_indices = list(range(0, len(keywords), step))[:5000]
                sample_keywords = [keywords[i] for i in sample_indices]
                
                progress_bar = st.progress(0)
                st.info("Procesando embeddings con OpenAI (esto puede tomar unos minutos)...")
                
                # Crear embeddings para muestra
                response = client.embeddings.create(
                    model="text-embedding-3-small",
                    input=sample_keywords
                )
                progress_bar.progress(0.5)
                
# Extraer embeddings
                sample_embeddings = np.array([item.embedding for item in response.data])
                
                # Propagar embeddings al resto por similitud TF-IDF
                st.info("Propagando embeddings al resto de keywords...")
                vectorizer = TfidfVectorizer()
                tfidf_matrix = vectorizer.fit_transform(keywords)
                
                all_embeddings = np.zeros((len(keywords), len(sample_embeddings[0])))
                # Asignar embeddings a la muestra
                for i, idx in enumerate(sample_indices):
                    all_embeddings[idx] = sample_embeddings[i]
                
                # Para el resto, encontrar el m√°s similar en TF-IDF
                remaining_indices = [i for i in range(len(keywords)) if i not in sample_indices]
                for i, idx in enumerate(remaining_indices):
                    similarities = cosine_similarity(
                        tfidf_matrix[idx:idx+1],
                        tfidf_matrix[sample_indices]
                    )[0]
                    most_similar_idx = sample_indices[np.argmax(similarities)]
                    all_embeddings[idx] = all_embeddings[most_similar_idx]
                    
                    # Actualizar progreso para la segunda mitad
                    if i % 100 == 0:
                        progress_bar.progress(0.5 + min(0.5, (i / len(remaining_indices) * 0.5)))
                
                progress_bar.progress(1.0)
            else:
                # Si son menos de 5000, procesar todas
                progress_bar = st.progress(0)
                st.info(f"Procesando embeddings para todas las {len(keywords)} keywords con OpenAI...")
                
                # Procesar en lotes de 1000 para evitar l√≠mites de API
                batch_size = 1000
                for i in range(0, len(keywords), batch_size):
                    batch_end = min(i + batch_size, len(keywords))
                    batch = keywords[i:batch_end]
                    
                    response = client.embeddings.create(
                        model="text-embedding-3-small",
                        input=batch
                    )
                    batch_embeddings = [item.embedding for item in response.data]
                    all_embeddings.extend(batch_embeddings)
                    
                    progress_bar.progress(min(1.0, batch_end / len(keywords)))
                
                progress_bar.progress(1.0)
                
            embeddings = np.array(all_embeddings) if isinstance(all_embeddings, list) else all_embeddings
            st.success(f"‚úÖ Generados embeddings de {embeddings.shape[1]} dimensiones usando OpenAI")
            return embeddings
                
        except Exception as e:
            st.error(f"Error generando embeddings con OpenAI: {str(e)}")
            st.info("Intentando con Sentence Transformers como alternativa...")
# Opci√≥n 2: Usar Sentence Transformers como fallback (sin costo)
    if sentence_transformers_available:
        try:
            st.success("Usando SentenceTransformer como fallback (sin costo)")
            
            # Usa un modelo multiling√ºe si tus keywords est√°n en varios idiomas
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            
            progress_bar = st.progress(0)
            keywords = df['keyword_processed'].fillna('').tolist()
            
            # Procesar en batches para evitar problemas de memoria
            batch_size = 512
            all_embeddings = []
            
            for i in range(0, len(keywords), batch_size):
                batch = keywords[i:i+batch_size]
                batch_embeddings = model.encode(batch, show_progress_bar=False)
                all_embeddings.extend(batch_embeddings)
                progress_bar.progress(min(1.0, (i + batch_size) / len(keywords)))
                
            progress_bar.progress(1.0)
            embeddings = np.array(all_embeddings)
            st.success(f"‚úÖ Generados embeddings de {embeddings.shape[1]} dimensiones usando SentenceTransformer")
            return embeddings
        except Exception as e:
            st.error(f"Error con SentenceTransformer: {str(e)}")
    
    # Opci√≥n 3: Fallback a TF-IDF (menos preciso) como √∫ltimo recurso
    st.warning("Usando TF-IDF como √∫ltimo recurso (menos preciso sem√°nticamente)")
    return generate_tfidf_embeddings(df['keyword_processed'].fillna(''))

# Funci√≥n original TF-IDF como fallback
def generate_tfidf_embeddings(texts, min_df=1, max_df=0.95):
    st.info("Generando vectores TF-IDF para las keywords...")
    progress_bar = st.progress(0)
    
    try:
        # Crear un vectorizador con par√°metros configurables
        vectorizer = TfidfVectorizer(
            max_features=300,  # Limitar caracter√≠sticas para prevenir problemas de memoria
            min_df=min_df,     # Ignorar t√©rminos que aparecen en menos de N documentos
            max_df=max_df,     # Ignorar t√©rminos que aparecen en m√°s del N% de los documentos
            stop_words='english'
        )
        
        # Asegurar que no hay valores nulos
        clean_texts = [t if isinstance(t, str) and t else " " for t in texts]
        
        # Generar matriz TF-IDF
        progress_bar.progress(0.3)
        tfidf_matrix = vectorizer.fit_transform(clean_texts)
        progress_bar.progress(0.8)
        
        # Convertir a array denso
        embeddings = tfidf_matrix.toarray()
        progress_bar.progress(1.0)
        
        st.success(f"‚úÖ Generados {embeddings.shape[1]} vectores TF-IDF")
        return embeddings
    except Exception as e:
        st.error(f"Error generando embeddings TF-IDF: {str(e)}")
        # √öltimo recurso: generar vectores aleatorios
        st.warning("Generando vectores aleatorios como √∫ltimo recurso")
        random_embeddings = np.random.rand(len(texts), 100)
        return random_embeddings
# MEJORA 2: Algoritmo de clustering mejorado
def improved_clustering(embeddings, num_clusters=None, min_cluster_size=5):
    st.info("Aplicando algoritmos de clustering avanzados...")
    
    # Determinar autom√°ticamente el n√∫mero √≥ptimo de clusters si no se especifica
    if num_clusters is None:
        try:
            from sklearn.metrics import silhouette_score
            
            st.info("Buscando n√∫mero √≥ptimo de clusters...")
            sil_scores = []
            max_clusters = min(30, len(embeddings) // 5)
            range_n_clusters = range(2, max(3, max_clusters))
            
            progress_bar = st.progress(0)
            
            # Calcular score de silueta para diferentes n√∫meros de clusters
            for i, n_clusters in enumerate(range_n_clusters):
                # Usar K-Means para la prueba por ser m√°s r√°pido
                from sklearn.cluster import KMeans
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=2)
                cluster_labels = kmeans.fit_predict(embeddings)
                
                # Calcular silueta (si hay suficientes muestras)
                if len(set(cluster_labels)) > 1:
                    try:
                        # Usar muestra para calcular silueta si hay muchos datos
                        if len(embeddings) > 5000:
                            sample_indices = np.random.choice(len(embeddings), 5000, replace=False)
                            sample_score = silhouette_score(
                                embeddings[sample_indices], 
                                cluster_labels[sample_indices]
                            )
                        else:
                            sample_score = silhouette_score(embeddings, cluster_labels)
                        sil_scores.append(sample_score)
                    except:
                        sil_scores.append(0)
                else:
                    sil_scores.append(0)
                    
                progress_bar.progress((i + 1) / len(range_n_clusters))
                    
            # Seleccionar el n√∫mero de clusters con mejor score
            if sil_scores:
                best_num_clusters = range_n_clusters[np.argmax(sil_scores)]
                st.success(f"N√∫mero √≥ptimo de clusters determinado: {best_num_clusters}")
                num_clusters = best_num_clusters
            else:
                st.warning("No se pudo determinar el n√∫mero √≥ptimo de clusters. Usando valor predeterminado.")
        except Exception as e:
            st.error(f"Error determinando n√∫mero √≥ptimo de clusters: {str(e)}")
    
    # Probar HDBSCAN si est√° disponible (mejor para clusters de forma irregular)
    if hdbscan_available:
        try:
            st.info("Aplicando HDBSCAN para detecci√≥n de clusters de forma natural...")
            
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=1,
                cluster_selection_epsilon=0.5,
                metric='euclidean',
                cluster_selection_method='eom'
            )
            
            cluster_labels = clusterer.fit_predict(embeddings)
            
            # Verificar si HDBSCAN encontr√≥ una estructura razonable
            # Limitar el n√∫mero m√°ximo de clusters a un valor razonable
            unique_clusters = np.unique(cluster_labels)
            non_noise_clusters = [c for c in unique_clusters if c != -1]
            
            if len(non_noise_clusters) > 1 and len(non_noise_clusters) <= num_clusters * 2:
                st.success(f"HDBSCAN identific√≥ {len(non_noise_clusters)} clusters naturales")
                
                # Reasignar cluster -1 (ruido) al cluster m√°s cercano
                if -1 in unique_clusters:
                    noise_indices = np.where(cluster_labels == -1)[0]
                    for idx in noise_indices:
                        # Encontrar el centroide m√°s cercano
                        min_dist = float('inf')
                        nearest_cluster = non_noise_clusters[0]
                        
                        for cluster in non_noise_clusters:
                            cluster_points = embeddings[cluster_labels == cluster]
                            if len(cluster_points) > 0:
                                centroid = np.mean(cluster_points, axis=0)
                                dist = np.linalg.norm(embeddings[idx] - centroid)
                                if dist < min_dist:
                                    min_dist = dist
                                    nearest_cluster = cluster
                        
                        cluster_labels[idx] = nearest_cluster
                
                # Reasignar IDs para que empiecen desde 1
                old_to_new = {old_id: new_id + 1 for new_id, old_id in enumerate(np.unique(cluster_labels))}
                cluster_labels = np.array([old_to_new[label] for label in cluster_labels])
                
                return cluster_labels
        except Exception as e:
            st.warning(f"Error con HDBSCAN: {str(e)}. Usando clustering jer√°rquico.")
# Fallback a clustering jer√°rquico
    try:
        st.info("Aplicando clustering jer√°rquico aglomerativo...")
        # Probar diferentes m√©todos de linkage para encontrar el mejor
        methods = ['ward', 'complete', 'average']
        best_method = 'ward'  # Valor predeterminado
        
        # Si el dataset no es demasiado grande, probar diferentes m√©todos
        if len(embeddings) < 5000:
            coherence_scores = []
            
            for method in methods:
                try:
                    Z = linkage(embeddings, method=method)
                    labels = fcluster(Z, t=num_clusters, criterion="maxclust")
                    
                    # Calcular coherencia promedio
                    coherence = 0
                    for cluster_id in np.unique(labels):
                        cluster_vectors = embeddings[labels == cluster_id]
                        if len(cluster_vectors) > 1:
                            centroid = np.mean(cluster_vectors, axis=0)
                            dists = np.linalg.norm(cluster_vectors - centroid, axis=1)
                            coherence += np.mean(1 / (1 + dists))
                    
                    coherence_scores.append(coherence / len(np.unique(labels)))
                except:
                    coherence_scores.append(0)
            
            if coherence_scores:
                best_method = methods[np.argmax(coherence_scores)]
                st.success(f"M√©todo de linkage √≥ptimo: {best_method}")
        
        # Aplicar clustering con el mejor m√©todo
        Z = linkage(embeddings, method=best_method)
        labels = fcluster(Z, t=num_clusters, criterion="maxclust")
        
        return labels
        
    except Exception as e:
        st.error(f"Error en clustering jer√°rquico: {str(e)}")
        
        # √öltimo recurso: K-Means
        st.warning("Usando K-Means como alternativa")
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        return kmeans.fit_predict(embeddings) + 1  # +1 para empezar desde 1

# MEJORA 3: Refinamiento Post-Clustering
def refine_clusters(df, embeddings, original_cluster_column='cluster_id'):
    """Refina los clusters identificando y corrigiendo asignaciones pobres"""
    st.info("Refinando clusters para mejorar coherencia sem√°ntica...")
    
    # Guardar las asignaciones originales
    df['original_cluster'] = df[original_cluster_column]
    
    # 1. Identificar outliers sem√°nticos en cada cluster
    outliers = []
    for cluster_id in df[original_cluster_column].unique():
        # Obtener √≠ndices de este cluster
        cluster_indices = df[df[original_cluster_column] == cluster_id].index.tolist()
        
        if len(cluster_indices) <= 3:  # Clusters muy peque√±os, no refinar
            continue
            
        # Calcular centroide del cluster
        cluster_embeddings = np.array([embeddings[i] for i in cluster_indices])
        centroid = np.mean(cluster_embeddings, axis=0)
        
        # Calcular distancias al centroide
        distances = [np.linalg.norm(embeddings[i] - centroid) for i in cluster_indices]
        
        # Normalizar distancias para este cluster
        mean_dist = np.mean(distances)
        std_dist = np.std(distances)
        if std_dist == 0:
            continue
            
        normalized_distances = [(d - mean_dist) / std_dist for d in distances]
        
# Identificar outliers (keywords muy lejanas al centroide)
        for i, norm_dist in enumerate(normalized_distances):
            if norm_dist > 2.0:  # M√°s de 2 desviaciones est√°ndar
                outliers.append((cluster_indices[i], cluster_id, norm_dist))
    
    # 2. Reasignar outliers a clusters m√°s apropiados
    reassigned = 0
    for idx, original_cluster, _ in outliers:
        keyword_embedding = embeddings[idx]
        
        # Encontrar cluster m√°s cercano (excluyendo el original)
        min_distance = float('inf')
        best_cluster = original_cluster
        
        for cluster_id in df[original_cluster_column].unique():
            if cluster_id == original_cluster:
                continue
                
            # Obtener √≠ndices de este cluster
            cluster_indices = df[df[original_cluster_column] == cluster_id].index.tolist()
            
            # Calcular centroide
            cluster_embeddings = np.array([embeddings[i] for i in cluster_indices])
            centroid = np.mean(cluster_embeddings, axis=0)
            
            # Calcular distancia
            distance = np.linalg.norm(keyword_embedding - centroid)
            
            if distance < min_distance:
                min_distance = distance
                best_cluster = cluster_id
        
        # Reasignar si encontramos un cluster mejor
        if best_cluster != original_cluster:
            df.loc[idx, original_cluster_column] = best_cluster
            reassigned += 1
# 3. Combinar clusters demasiado similares
    similar_pairs = []
    clusters = df[original_cluster_column].unique()
    
    for i, cluster1 in enumerate(clusters):
        for cluster2 in clusters[i+1:]:
            # Calcular centroides
            indices1 = df[df[original_cluster_column] == cluster1].index.tolist()
            indices2 = df[df[original_cluster_column] == cluster2].index.tolist()
            
            if len(indices1) < 3 or len(indices2) < 3:
                continue  # Ignorar clusters muy peque√±os
                
            centroid1 = np.mean(np.array([embeddings[i] for i in indices1]), axis=0)
            centroid2 = np.mean(np.array([embeddings[i] for i in indices2]), axis=0)
            
            # Calcular similitud coseno
            similarity = np.dot(centroid1, centroid2) / (np.linalg.norm(centroid1) * np.linalg.norm(centroid2))
            
            if similarity > 0.8:  # Umbral alto para fusionar
                similar_pairs.append((cluster1, cluster2, similarity))
    
    # Ordenar por similitud para combinar primero los m√°s similares
    similar_pairs.sort(key=lambda x: x[2], reverse=True)
    
    # Combinar clusters (manteniendo el ID m√°s bajo)
    clusters_merged = 0
    processed_clusters = set()
    
    for cluster1, cluster2, _ in similar_pairs:
        if cluster1 in processed_clusters or cluster2 in processed_clusters:
            continue  # Evitar combinar clusters ya procesados
            
        # Elegir el ID m√°s bajo para mantener
        keep_id = min(cluster1, cluster2)
        remove_id = max(cluster1, cluster2)
        
        # Reasignar keywords del cluster a eliminar
        df.loc[df[original_cluster_column] == remove_id, original_cluster_column] = keep_id
        
        processed_clusters.add(remove_id)
        clusters_merged += 1
        
        # Limitar n√∫mero de fusiones
        if clusters_merged >= len(clusters) // 4:  # M√°ximo 25% de fusiones
            break
    
    st.success(f"Refinamiento completado: {reassigned} keywords reasignadas, {clusters_merged} clusters fusionados.")
    return df

# Funci√≥n para generar nombres de clusters con OpenAI
def generate_cluster_names(clusters_with_representatives, client, model="gpt-3.5-turbo"):
    if not clusters_with_representatives:
        return {}

    results = {}
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    progress_text.text("Generando nombres y descripciones para los clusters...")

    # Process clusters in smaller batches to manage costs
    batch_size = 5
    total_batches = (len(clusters_with_representatives) + batch_size - 1) // batch_size
    
    for batch_idx, batch_start in enumerate(range(0, len(clusters_with_representatives), batch_size)):
        batch_end = min(batch_start + batch_size, len(clusters_with_representatives))
        batch_clusters = list(clusters_with_representatives.items())[batch_start:batch_end]

        try:
            # Prompt mejorado para analizar clusters
            analysis_prompt = """I'll provide representative keywords for several clusters. For each cluster, analyze the keywords to identify:
1. Common themes, topics, or categories
2. User intent or purpose behind these keywords
3. Semantic relationships between words
4. Any distinctive patterns that make this cluster unique

Be thorough and insightful in your analysis.

"""

            # Track the cluster order to match the response
            cluster_order = []
            for cluster_id, keywords in batch_clusters:
                cluster_order.append(cluster_id)
                analysis_prompt += f"Cluster {cluster_id} representative keywords: {', '.join(keywords[:20])}\n\n"

            analysis_response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": analysis_prompt}],
                temperature=0.1,
                max_tokens=500
            )

            analysis_text = analysis_response.choices[0].message.content.strip()

            # Prompt mejorado para generar nombres m√°s precisos
            naming_prompt = f"""Based on the following analysis of keyword clusters, provide a specific name and description for each cluster.

Analysis:
{analysis_text}

For each cluster, provide:
1. A specific, descriptive cluster name (3-5 words) that clearly identifies the semantic theme
2. A concise description (1-2 sentences) that accurately represents the semantic relationship between the keywords

Your names should be concrete and specific, not generic. Focus on semantic meaning, not just superficial word patterns.

Format your response as a JSON array, with each element containing cluster_id, cluster_name, and description for clusters {', '.join(map(str, cluster_order))}.
"""

            # Conditional parameter setting based on model
            completion_params = {
                "model": model,
                "messages": [{"role": "user", "content": naming_prompt}],
                "temperature": 0.2,
                "max_tokens": 600
            }
            
            # Only add response_format for gpt-3.5-turbo model
            if "gpt-3.5-turbo" in model:
                completion_params["response_format"] = {"type": "json_object"}
                
            naming_response = client.chat.completions.create(**completion_params)
            naming_text = naming_response.choices[0].message.content.strip()
# Procesar la respuesta JSON
            try:
                # For GPT-4, we need to extract the JSON from the response text
                # Find JSON-like content between curly braces
                import re
                import json
                
                # Try direct JSON parsing first
                try:
                    data = json.loads(naming_text)
                except json.JSONDecodeError:
                    # If direct parsing fails, try to extract JSON-like content
                    json_match = re.search(r'({.*})', naming_text, re.DOTALL)
                    if json_match:
                        try:
                            data = json.loads(json_match.group(1))
                        except json.JSONDecodeError:
                            # If still failing, try a more lenient approach - look for an array
                            array_match = re.search(r'(\[.*\])', naming_text, re.DOTALL)
                            if array_match:
                                try:
                                    # Try parsing as an array and then convert to expected format
                                    array_data = json.loads(array_match.group(1))
                                    data = {"clusters": array_data}
                                except json.JSONDecodeError:
                                    raise ValueError("Could not extract valid JSON from response")
                            else:
                                raise ValueError("Could not extract valid JSON from response")
                    else:
                        raise ValueError("Could not extract valid JSON from response")

                # Handle different JSON structures that might be returned
                clusters_data = None
                if "clusters" in data:
                    clusters_data = data["clusters"]
                elif isinstance(data, list):
                    clusters_data = data
                else:
                    # Try to find any list in the response
                    for key, value in data.items():
                        if isinstance(value, list) and len(value) > 0:
                            clusters_data = value
                            break

                if clusters_data:
                    # Match clusters by position if IDs don't match
                    if len(clusters_data) == len(cluster_order):
                        for i, cluster_info in enumerate(clusters_data):
                            actual_id = cluster_order[i]
                            results[actual_id] = (
                                cluster_info.get("cluster_name", f"Cluster {actual_id}"),
                                cluster_info.get("description", "No description provided.")
                            )
            except Exception as e:
                st.warning(f"Error analizando respuesta JSON para batch {batch_idx+1}/{total_batches}: {str(e)}")
                # Fallback para este batch: intentar extraer manualmente la informaci√≥n
                try:
                    # Fallback more manual parsing for both models
                    for i, cluster_id in enumerate(cluster_order):
                        # Look for "Cluster {cluster_id}" or similar pattern in the text
                        cluster_section = re.search(f"Cluster {cluster_id}[:\s]*(.*?)(?:Cluster \d|$)", 
                                                   naming_text, re.DOTALL | re.IGNORECASE)
                        
                        if cluster_section:
                            section_text = cluster_section.group(1).strip()
                            # Try to extract name and description
                            name_match = re.search(r"name[:\s]*(.*?)(?:description|\n|$)", 
                                                 section_text, re.DOTALL | re.IGNORECASE)
                            desc_match = re.search(r"description[:\s]*(.*?)(?:\n\n|$)", 
                                                  section_text, re.DOTALL | re.IGNORECASE)
                            
                            name = name_match.group(1).strip() if name_match else f"Cluster {cluster_id}"
                            desc = desc_match.group(1).strip() if desc_match else f"Grupo de keywords {cluster_id}"
                            
                            results[cluster_id] = (name, desc)
                        else:
                            results[cluster_id] = (f"Cluster {cluster_id}", f"Grupo de keywords {cluster_id}")
                except Exception:
                    # Ultimate fallback if all parsing fails
                    for cluster_id in cluster_order:
                        results[cluster_id] = (f"Cluster {cluster_id}", f"Grupo de keywords {cluster_id}")

            # Progress update
            progress_bar.progress((batch_idx + 1) / total_batches)
            time.sleep(1)  # Evitar l√≠mites de rate

        except Exception as e:
            st.warning(f"Error generando nombres para batch {batch_idx+1}/{total_batches}: {str(e)}")
            # Proporcionar nombres predeterminados
            for cluster_id, _ in batch_clusters:
                results[cluster_id] = (f"Cluster {cluster_id}", "Error generating description")
    
    # Asegurar que todos los clusters tienen nombre
    for cluster_id in clusters_with_representatives.keys():
        if cluster_id not in results:
            results[cluster_id] = (f"Cluster {cluster_id}", f"Grupo de keywords {cluster_id}")
    
    progress_bar.progress(1.0)
    progress_text.text(f"‚úÖ Nombres y descripciones generados para {len(results)} clusters")
    
    return results
# MEJORA 5: Evaluaci√≥n avanzada de clusters
def evaluate_cluster_quality(df, embeddings, cluster_column='cluster_id'):
    """Eval√∫a la calidad de los clusters usando m√∫ltiples m√©tricas"""
    st.subheader("Evaluaci√≥n Avanzada de Calidad de Clusters")
    
    metrics = {
        'silhouette': [],
        'density': [],
        'separation': [],
        'coherence': []
    }
    
    # Calcular centroides de todos los clusters
    centroids = {}
    for cluster_id in df[cluster_column].unique():
        indices = df[df[cluster_column] == cluster_id].index.tolist()
        centroids[cluster_id] = np.mean(np.array([embeddings[i] for i in indices]), axis=0)
    
    # Evaluar cada cluster
    cluster_progress = st.progress(0)
    for i, cluster_id in enumerate(df[cluster_column].unique()):
        indices = df[df[cluster_column] == cluster_id].index.tolist()
        cluster_vectors = np.array([embeddings[i] for i in indices])
        centroid = centroids[cluster_id]
        
        # 1. Densidad (distancia promedio al centro)
        distances = [np.linalg.norm(vec - centroid) for vec in cluster_vectors]
        density = 1 / (1 + np.mean(distances)) if distances else 0
        metrics['density'].append((cluster_id, density))
        
        # 2. Coherencia (similitud coseno promedio entre vectores)
        coherence = calculate_cluster_coherence(cluster_vectors)
        metrics['coherence'].append((cluster_id, coherence))
        
        # 3. Separaci√≥n (distancia m√≠nima a otro centroide)
        min_separation = float('inf')
        for other_id, other_centroid in centroids.items():
            if other_id != cluster_id:
                separation = np.linalg.norm(centroid - other_centroid)
                min_separation = min(min_separation, separation)
        
        if min_separation != float('inf'):
            metrics['separation'].append((cluster_id, min_separation))
            
        cluster_progress.progress((i + 1) / len(df[cluster_column].unique()))
    
    # Visualizar m√©tricas
    col1, col2 = st.columns(2)
    
    with col1:
        # Gr√°fico de coherencia vs tama√±o
        coherence_data = pd.DataFrame(metrics['coherence'], columns=['cluster_id', 'score'])
        coherence_data = coherence_data.merge(
            df.groupby(cluster_column)['keyword'].count().reset_index(),
            left_on='cluster_id', right_on=cluster_column
        )
        coherence_data = coherence_data.merge(
            df.drop_duplicates(cluster_column)[['cluster_id', 'cluster_name']],
            on='cluster_id'
        )
        
        fig = px.scatter(
            coherence_data, 
            x='score', 
            y='keyword', 
            color='score',
            size='keyword',
hover_data=['cluster_name'],
            labels={
                'score': 'Coherencia Sem√°ntica', 
                'keyword': 'Tama√±o del Cluster'
            },
            title='Relaci√≥n entre Coherencia y Tama√±o',
            color_continuous_scale='Blues'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Gr√°fico de separaci√≥n vs densidad
        if metrics['separation']:
            separation_data = pd.DataFrame(metrics['separation'], columns=['cluster_id', 'separation'])
            density_data = pd.DataFrame(metrics['density'], columns=['cluster_id', 'density'])
            
            combined_data = separation_data.merge(density_data, on='cluster_id')
            combined_data = combined_data.merge(
                df.drop_duplicates(cluster_column)[['cluster_id', 'cluster_name']],
                on='cluster_id'
            )
            
            fig2 = px.scatter(
                combined_data,
                x='separation',
                y='density',
                color='density',
                hover_data=['cluster_name'],
                labels={
                    'separation': 'Separaci√≥n entre Clusters',
                    'density': 'Densidad del Cluster'
                },
                title='Separaci√≥n vs Densidad',
                color_continuous_scale='Greens'
            )
            st.plotly_chart(fig2, use_container_width=True)
    
    # Identificar clusters problem√°ticos
    st.subheader("Diagn√≥stico de Clusters")
    
    # Calcular umbrales
    coherence_threshold = np.percentile([x[1] for x in metrics['coherence']], 25)
    problematic = [x[0] for x in metrics['coherence'] if x[1] < coherence_threshold]
    
    # A√±adir coherencia al dataframe original
    for cluster_id, coherence in metrics['coherence']:
        df.loc[df[cluster_column] == cluster_id, 'cluster_coherence'] = coherence
    
    if problematic:
        st.warning(f"Clusters con baja coherencia sem√°ntica: {problematic}")
        st.info("""
        Recomendaciones para mejorar:
        - Considera aumentar el n√∫mero de clusters
        - Revisa las keywords en estos clusters espec√≠ficos
        - Prueba usar embeddings de mayor calidad
        - Considera la posibilidad de dividir estos clusters manualmente
        """)
    else:
        st.success("Todos los clusters tienen buena coherencia sem√°ntica")
        
    return df

# Funci√≥n b√°sica para calcular coherencia
def calculate_cluster_coherence(cluster_embeddings):
    """Calculate semantic coherence of a cluster based on embedding similarity"""
    if len(cluster_embeddings) <= 1:
        return 1.0  # Perfect coherence for single element

    try:
        # Calculate centroid
        centroid = np.mean(cluster_embeddings, axis=0)

        # Calculate average cosine similarity to centroid
        similarities = []
        for emb in cluster_embeddings:
            # Evitar divisiones por cero
            norm_emb = np.linalg.norm(emb)
            norm_centroid = np.linalg.norm(centroid)
            if norm_emb > 0 and norm_centroid > 0:
                similarity = np.dot(emb, centroid) / (norm_emb * norm_centroid)
                similarities.append(similarity)
            else:
                similarities.append(0.0)

        return np.mean(similarities) if similarities else 0.0
    except Exception as e:
        st.warning(f"Error calculando coherencia: {str(e)}")
        return 0.5  # Valor predeterminado en caso de error
# Funci√≥n principal para ejecutar el clustering mejorado
def run_clustering(uploaded_file, openai_api_key, num_clusters, pca_variance, max_pca_components, min_df, max_df, gpt_model):
    """Ejecuta el proceso completo de clustering y devuelve los resultados"""
    if uploaded_file is None:
        st.warning("Por favor, sube un archivo CSV con keywords.")
        return False, None
    
    st.info("Iniciando proceso de clustering sem√°ntico avanzado...")
    
    # Configurar cliente OpenAI si se proporciona la clave API
    client = None
    if openai_api_key and openai_available:
        try:
            # Verificamos que la API key no est√© vac√≠a
            if openai_api_key.strip() == "":
                st.info("No se ha proporcionado una API Key de OpenAI v√°lida. Los clusters tendr√°n nombres gen√©ricos.")
            else:
                # Establecemos la API key como variable de entorno
                os.environ["OPENAI_API_KEY"] = openai_api_key
                
                # Creamos el cliente sin par√°metros adicionales
                client = OpenAI()
                
                # Verificamos la conexi√≥n con una solicitud simple
                try:
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": "Test"}],
                        max_tokens=5
                    )
                    st.success("‚úÖ Conexi√≥n con OpenAI establecida correctamente")
                except Exception as e:
                    st.error(f"Error al verificar la conexi√≥n con OpenAI: {str(e)}")
                    st.error("Posible causa: API Key inv√°lida o problemas de conexi√≥n")
                    client = None
        except Exception as e:
            st.error(f"Error configurando cliente OpenAI: {str(e)}")
            st.info("Continuando sin funcionalidades de OpenAI")
            client = None
    elif not openai_available:
        st.warning("Biblioteca OpenAI no est√° disponible. Continuando sin funcionalidades de OpenAI.")
    elif not openai_api_key or openai_api_key.strip() == "":
        st.info("No se ha proporcionado API Key de OpenAI. Se usar√° SentenceTransformers como alternativa gratuita.")
    
    try:
        # Cargar y procesar el CSV
        try:
            df = pd.read_csv(uploaded_file, header=None, names=["keyword"])
            num_keywords = len(df)
            st.success(f"‚úÖ Cargadas {num_keywords} keywords del archivo CSV")
            
            # Mostrar estimaci√≥n de costes basada en el CSV cargado
            show_csv_cost_estimate(num_keywords, gpt_model, num_clusters)
            
        except Exception as e:
            st.error(f"Error leyendo CSV: {str(e)}")
            st.info("Intentando formato alternativo...")
            # Intentar otros formatos/separadores
            try:
                content = uploaded_file.getvalue().decode('utf-8')
                df = pd.read_csv(StringIO(content), sep=None, engine='python', header=None)
                df.columns = ["keyword"]
                num_keywords = len(df)
                st.success(f"‚úÖ Cargadas {num_keywords} keywords del archivo CSV (formato alternativo)")
                
                # Mostrar estimaci√≥n de costes basada en el CSV cargado
                show_csv_cost_estimate(num_keywords, gpt_model, num_clusters)
                
            except Exception as e2:
                st.error(f"No se pudo leer el archivo CSV: {str(e2)}")
                return False, None
# Preprocesar keywords
        st.subheader("Preprocesamiento de Keywords")
        st.info("Preprocesando keywords con an√°lisis sem√°ntico mejorado...")
        
        # MEJORA 4: Usar el preprocesamiento sem√°ntico mejorado
        use_advanced = spacy_available
        if use_advanced:
            st.success("Usando preprocesamiento avanzado con an√°lisis ling√º√≠stico")
        else:
            st.info("Usando preprocesamiento est√°ndar (SpaCy no disponible)")
            
        keywords_processed = preprocess_keywords(df["keyword"].tolist(), use_advanced=use_advanced)
        df['keyword_processed'] = keywords_processed
        st.success("‚úÖ Keywords preprocesadas correctamente")
        
        # Generar embeddings mejorados
        st.subheader("Generaci√≥n de Vectores Sem√°nticos")
        
        # MEJORA 1: Usar embeddings de alta calidad
        keyword_embeddings = generate_embeddings(df, openai_available, openai_api_key)
        
        # Aplicar PCA si los embeddings son de alta dimensionalidad
        if keyword_embeddings.shape[1] > max_pca_components:
            st.subheader("Reducci√≥n de Dimensionalidad (PCA)")
            
            try:
                pca_progress = st.progress(0)
                pca_text = st.empty()
                pca_text.text("Analizando varianza explicada...")
                
                # Determine optimal number of components experimentally
                pca = PCA()
                pca.fit(keyword_embeddings)
                cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
                pca_progress.progress(0.3)
                
                # Find the number of components that explain the desired variance
                target_variance = pca_variance / 100.0
                n_components = np.argmax(cumulative_variance >= target_variance) + 1
                # Si no hay suficientes componentes para la varianza deseada, usar el m√°ximo
                if n_components == 1 and len(cumulative_variance) > 1:
                    n_components = min(max_pca_components, len(cumulative_variance))
                    
                pca_text.text(f"Componentes para {pca_variance}% de varianza: {n_components}")
                pca_progress.progress(0.6)
                
                # Use that number (with a reasonable cap)
                max_components = min(n_components, max_pca_components)
                pca = PCA(n_components=max_components)
                keyword_embeddings_reduced = pca.fit_transform(keyword_embeddings)
                
                pca_progress.progress(1.0)
                pca_text.text(f"‚úÖ PCA aplicado: {max_components} dimensiones ({pca_variance}% de varianza explicada)")
            except Exception as e:
                st.error(f"Error aplicando PCA: {str(e)}")
                st.info("Continuando sin reducci√≥n de dimensionalidad")
                # En caso de error, mantener los embeddings originales
                keyword_embeddings_reduced = keyword_embeddings
        else:
            # No necesita PCA si la dimensionalidad ya es adecuada
            keyword_embeddings_reduced = keyword_embeddings
            st.info(f"Dimensionalidad de embeddings adecuada ({keyword_embeddings.shape[1]}). No se requiere PCA.")
# Aplicar clustering mejorado
        st.subheader("Clustering Sem√°ntico Avanzado")
        
        # MEJORA 2: Usar algoritmo de clustering mejorado
        try:
            cluster_labels = improved_clustering(keyword_embeddings_reduced, num_clusters=num_clusters)
            df["cluster_id"] = cluster_labels
            st.success(f"‚úÖ Keywords agrupadas en {len(df['cluster_id'].unique())} clusters sem√°nticos")
        except Exception as e:
            st.error(f"Error en clustering avanzado: {str(e)}")
            st.info("Intentando clustering alternativo...")
            
            # Fallback: Asignar clusters de manera m√°s b√°sica
            try:
                from sklearn.cluster import KMeans
                kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
                df["cluster_id"] = kmeans.fit_predict(keyword_embeddings_reduced) + 1
                st.success("‚úÖ Clustering completado usando K-Means como alternativa")
            except Exception as e2:
                st.error(f"Error en clustering alternativo: {str(e2)}")
                # √öltimo recurso: asignar clusters aleatorios
                df["cluster_id"] = np.random.randint(1, num_clusters + 1, size=len(df))
                st.warning("‚ö†Ô∏è Se han asignado clusters aleatorios como √∫ltimo recurso")
        
        # MEJORA 3: Refinar clusters
        st.subheader("Refinamiento de Clusters")
        df = refine_clusters(df, keyword_embeddings_reduced)
        num_clusters_after_refinement = len(df['cluster_id'].unique())
        st.success(f"‚úÖ Refinamiento completado: {num_clusters_after_refinement} clusters finales")
        
        # Identificar keywords representativas para cada cluster
        st.subheader("An√°lisis de Clusters")
        
        rep_progress = st.progress(0)
        rep_text = st.empty()
        rep_text.text("Identificando keywords representativas...")
        
        clusters_with_representatives = {}
        try:
            for i, cluster_num in enumerate(df['cluster_id'].unique()):
                cluster_size = len(df[df['cluster_id'] == cluster_num])
                n_representatives = min(20, cluster_size)
                
                # Get indices of keywords in this cluster
                indices = df[df['cluster_id'] == cluster_num].index.tolist()
                
                # Calculate centroid of the cluster
                cluster_embeddings = np.array([keyword_embeddings_reduced[i] for i in indices])
                centroid = np.mean(cluster_embeddings, axis=0)
                
                # Calculate distance to centroid for each keyword
                distances = [np.linalg.norm(keyword_embeddings_reduced[i] - centroid) for i in indices]
                
                # Get indices of keywords closest to centroid
                sorted_indices = np.argsort(distances)[:n_representatives]
                representative_indices = [indices[i] for i in sorted_indices]
                representative_keywords = df.iloc[representative_indices]['keyword'].tolist()
                
                clusters_with_representatives[cluster_num] = representative_keywords
                
                # Update progress
                rep_progress.progress((i+1) / len(df['cluster_id'].unique()))
            
            rep_progress.progress(1.0)
            rep_text.text(f"‚úÖ Identificadas keywords representativas para {len(clusters_with_representatives)} clusters")
        except Exception as e:
            st.error(f"Error identificando keywords representativas: {str(e)}")
            # Fallback: tomar las primeras N keywords de cada cluster
            for cluster_num in df['cluster_id'].unique():
                cluster_keywords = df[df['cluster_id'] == cluster_num]['keyword'].tolist()
                clusters_with_representatives[cluster_num] = cluster_keywords[:min(20, len(cluster_keywords))]
            st.warning("Se han seleccionado keywords representativas b√°sicas como alternativa")
# Generar nombres para los clusters si est√° disponible OpenAI
        if client:
            st.subheader("Generaci√≥n de Nombres para Clusters")
            try:
                cluster_names = generate_cluster_names(
                    clusters_with_representatives, 
                    client,
                    model=gpt_model
                )
            except Exception as e:
                st.error(f"Error generando nombres de clusters: {str(e)}")
                cluster_names = {k: (f"Cluster {k}", f"Grupo de keywords {k}") for k in df['cluster_id'].unique()}
        else:
            st.warning("No se pueden generar nombres de clusters sin API Key de OpenAI")
            cluster_names = {k: (f"Cluster {k}", f"Grupo de keywords {k}") for k in df['cluster_id'].unique()}
        
        # Aplicar resultados al DataFrame
        df['cluster_name'] = ''
        df['cluster_description'] = ''
        df['representative'] = False
        
        for cluster_num, (name, description) in cluster_names.items():
            df.loc[df['cluster_id'] == cluster_num, 'cluster_name'] = name
            df.loc[df['cluster_id'] == cluster_num, 'cluster_description'] = description
            
            # Marcar keywords representativas
            for keyword in clusters_with_representatives.get(cluster_num, []):
                matching_indices = df[(df['cluster_id'] == cluster_num) & (df['keyword'] == keyword)].index
                if not matching_indices.empty:
                    df.loc[matching_indices, 'representative'] = True
        
        # MEJORA 5: Evaluaci√≥n avanzada de calidad de clusters
        df = evaluate_cluster_quality(df, keyword_embeddings_reduced)
        
        # Devolver los resultados
        return True, df
        
    except Exception as e:
        st.error(f"Error durante el proceso: {str(e)}")
        return False, None
#############################
# APLICACI√ìN PRINCIPAL
#############################

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Clustering Sem√°ntico Avanzado de Keywords",
    page_icon="üîç",
    layout="wide"
)

# Estilos CSS para mejorar la apariencia
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        margin-bottom: 0.5rem;
    }
    .info-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .success-box {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .highlight {
        background-color: #fffbcc;
        padding: 0.2rem 0.5rem;
        border-radius: 0.2rem;
    }
</style>
""", unsafe_allow_html=True)

# T√≠tulo y descripci√≥n
st.markdown("<div class='main-header'>Clustering Sem√°ntico Avanzado de Keywords</div>", unsafe_allow_html=True)
st.markdown("""
Esta aplicaci√≥n te permite agrupar keywords sem√°nticamente similares utilizando t√©cnicas avanzadas de NLP y clustering.
Sube tu archivo CSV con las keywords y configura los par√°metros para obtener clusters de alta correlaci√≥n sem√°ntica.
""")

# Mostrar estado de librer√≠as avanzadas
with st.expander("Estado de bibliotecas sem√°nticas", expanded=False):
    col1, col2, col3 = st.columns(3)
    with col1:
        if openai_available:
            st.success("‚úÖ OpenAI disponible (con API Key)")
        else:
            st.warning("‚ö†Ô∏è OpenAI no disponible")
            
        if sentence_transformers_available:
            st.success("‚úÖ SentenceTransformers disponible (sin costo)")
        else:
            st.warning("‚ö†Ô∏è SentenceTransformers no disponible")
            st.markdown("""
            Para instalar:
            ```
            pip install sentence-transformers
            ```
            """)
    
    with col2:
        if spacy_available:
            st.success("‚úÖ SpaCy disponible")
        else:
            st.warning("‚ö†Ô∏è SpaCy no disponible")
            
        if hdbscan_available:
            st.success("‚úÖ HDBSCAN disponible")
        else:
            st.warning("‚ö†Ô∏è HDBSCAN no disponible")
            
    with col3:
        # Informaci√≥n de instalaci√≥n
        st.info("""
        Para m√°s funcionalidades:
        ```
        pip install sentence-transformers spacy hdbscan
        python -m spacy download en_core_web_sm
        ```
        """)

# Inicializaci√≥n de sesi√≥n
if 'process_complete' not in st.session_state:
    st.session_state.process_complete = False
if 'df_results' not in st.session_state:
    st.session_state.df_results = None
# Sidebar para la configuraci√≥n
st.sidebar.markdown("<div class='sub-header'>Configuraci√≥n</div>", unsafe_allow_html=True)

# 1. Subir CSV
uploaded_file = st.sidebar.file_uploader("Sube tu archivo CSV de keywords", type=['csv'])

# 2. API Key de OpenAI (ajustado el mensaje para indicar preferencia)
openai_api_key = st.sidebar.text_input(
    "API Key de OpenAI (recomendado)",
    type="password", 
    help="Proporciona tu API Key de OpenAI para embeddings de alta calidad (hasta 5000 keywords). Si no se proporciona, se usar√° SentenceTransformers como alternativa gratuita."
)

# Mostrar estado de procesamiento sem√°ntico
if openai_available:
    if openai_api_key:
        st.sidebar.success("‚úÖ API Key proporcionada - Se usar√° OpenAI para embeddings de alta precisi√≥n")
    else:
        if sentence_transformers_available:
            st.sidebar.info("‚ÑπÔ∏è Sin API Key - Se usar√° SentenceTransformers como alternativa gratuita")
        else:
            st.sidebar.warning("‚ö†Ô∏è Sin API Key ni SentenceTransformers - Se usar√° TF-IDF (precisi√≥n reducida)")
else:
    if sentence_transformers_available:
        st.sidebar.info("‚ÑπÔ∏è OpenAI no disponible - Se usar√° SentenceTransformers como alternativa gratuita")
    else:
        st.sidebar.error("‚ùå M√©todos avanzados no disponibles - Se usar√° TF-IDF (precisi√≥n reducida)")

# 3. Par√°metros de clustering
st.sidebar.markdown("<div class='sub-header'>Par√°metros</div>", unsafe_allow_html=True)

# Panel de explicaci√≥n de par√°metros - Colocado antes de los sliders
with st.sidebar.expander("‚ÑπÔ∏è Gu√≠a de Par√°metros", expanded=False):
    st.markdown("""
    ### Gu√≠a de Par√°metros de Clustering
    
    Aqu√≠ encontrar√°s explicaciones sobre cada par√°metro y c√≥mo ajustarlo para obtener mejores resultados:
    
    #### N√∫mero de clusters
    **¬øQu√© es?** El n√∫mero de grupos en los que se dividir√°n tus keywords.
    
    **C√≥mo usarlo:** 
    - **‚Üë Aumentar** si necesitas una divisi√≥n m√°s detallada y espec√≠fica por temas.
    - **‚Üì Disminuir** si prefieres grupos m√°s generales y amplios.
    
    **Resultado:**
    - **Valores altos** (15-30): Muchos grupos peque√±os y muy espec√≠ficos.
    - **Valores bajos** (5-10): Pocos grupos pero m√°s amplios tem√°ticamente.
    - **Ideal:** Generalmente entre 8-15 para 1000 keywords. Aumenta proporcionalmente con la cantidad de keywords.
    
    ---
    
    #### Varianza explicada PCA (%)
    **¬øQu√© es?** Determina cu√°nta informaci√≥n original se conserva al simplificar los datos. Piensa en esto como el "nivel de detalle" que se mantiene.
    
    **C√≥mo usarlo:**
    - **‚Üë Aumentar** para mayor precisi√≥n y preservar m√°s matices sem√°nticos.
    - **‚Üì Disminuir** para acelerar el procesamiento con conjuntos grandes.
    
    **Resultado:**
    - **Valores altos** (95-99%): Mayor precisi√≥n sem√°ntica pero m√°s lento.
    - **Valores bajos** (80-90%): Procesamiento m√°s r√°pido pero puede perderse algunos matices.
    - **Ideal:** 90-95% ofrece un buen equilibrio entre precisi√≥n y velocidad.
    
    ---
    
    #### M√°ximo de componentes PCA
    **¬øQu√© es?** Limita la complejidad m√°xima del modelo de an√°lisis. Similar a establecer un l√≠mite para evitar un exceso de complejidad.
    
    **C√≥mo usarlo:**
    - **‚Üë Aumentar** para datasets grandes o con alta diversidad tem√°tica.
    - **‚Üì Disminuir** para datasets m√°s peque√±os o centrados en un solo tema.
    
    **Resultado:**
    - **Valores altos** (100-200): Captura m√°s relaciones complejas entre palabras.
    - **Valores bajos** (30-75): M√°s eficiente pero puede simplificar demasiado.
    - **Ideal:** Entre 75-100 para la mayor√≠a de casos.
    
    ---
    
    #### Frecuencia m√≠nima de t√©rminos
    **¬øQu√© es?** Ignora palabras que aparecen en muy pocas keywords. Ayuda a filtrar palabras raras o errores tipogr√°ficos.
    
    **C√≥mo usarlo:**
    - **‚Üë Aumentar** para eliminar t√©rminos poco comunes y posible ruido.
    - **‚Üì Disminuir** para incluir t√©rminos poco frecuentes que podr√≠an ser importantes.
    
    **Resultado:**
    - **Valores altos** (3-5): Elimina m√°s t√©rminos raros, clustering m√°s "limpio".
    - **Valores bajos** (1-2): Conserva t√©rminos poco comunes, puede mantener m√°s ruido.
    - **Ideal:** 1-2 para datasets peque√±os, 2-3 para datasets grandes (+5000 keywords).
    
    ---
    
    #### Frecuencia m√°xima de t√©rminos (%)
    **¬øQu√© es?** Ignora palabras que aparecen en un alto porcentaje de keywords. Similar a eliminar "palabras comod√≠n" que est√°n en todas partes.
    
    **C√≥mo usarlo:**
    - **‚Üë Aumentar** para incluir m√°s t√©rminos comunes.
    - **‚Üì Disminuir** para filtrar palabras demasiado gen√©ricas.
    
    **Resultado:**
    - **Valores altos** (90-100%): Incluye casi todos los t√©rminos, incluso los muy comunes.
    - **Valores bajos** (70-85%): Enfoque en palabras m√°s distintivas, ignorando las gen√©ricas.
    - **Ideal:** 85-95% funciona bien para la mayor√≠a de datasets.
    """)
    
    st.info("""
    **Consejo:** Si no est√°s seguro, mant√©n los valores predeterminados. La aplicaci√≥n est√° optimizada para funcionar bien con estos par√°metros en la mayor√≠a de los casos.
    
    Para datasets grandes (+5000 keywords), considera aumentar ligeramente el n√∫mero de clusters y reducir la varianza explicada PCA para mantener tiempos de procesamiento razonables.
    """)
# Sliders para los par√°metros con descripciones mejoradas
num_clusters = st.sidebar.slider(
    "N√∫mero de clusters", 
    min_value=2, 
    max_value=50, 
    value=10, 
    help="N√∫mero de grupos en los que se dividir√°n tus keywords. M√°s clusters = grupos m√°s espec√≠ficos."
)

pca_variance = st.sidebar.slider(
    "Varianza explicada PCA (%)", 
    min_value=50, 
    max_value=99, 
    value=95, 
    help="Porcentaje de informaci√≥n que se conserva. Mayor valor = mayor precisi√≥n pero m√°s lento."
)

max_pca_components = st.sidebar.slider(
    "M√°ximo de componentes PCA", 
    min_value=10, 
    max_value=300, 
    value=100, 
    help="L√≠mite de complejidad del modelo. Mayor valor = captura m√°s relaciones complejas."
)

# 4. Opciones avanzadas
st.sidebar.markdown("<div class='sub-header'>Opciones avanzadas</div>", unsafe_allow_html=True)

min_df = st.sidebar.slider(
    "Frecuencia m√≠nima de t√©rminos", 
    min_value=1, 
    max_value=10, 
    value=1, 
    help="Ignora t√©rminos poco frecuentes. Mayor valor = elimina m√°s palabras raras."
)

max_df = st.sidebar.slider(
    "Frecuencia m√°xima de t√©rminos (%)", 
    min_value=50, 
    max_value=100, 
    value=95, 
    help="Ignora t√©rminos demasiado comunes. Menor valor = elimina m√°s palabras gen√©ricas."
)

gpt_model = st.sidebar.selectbox(
    "Modelo para nombrar clusters", 
    ["gpt-3.5-turbo", "gpt-4"], 
    index=0,
    help="GPT-4 proporciona nombres m√°s precisos pero es m√°s costoso y lento."
)

# A√±adir calculadora de costes al sidebar (donde el usuario puede simular diferentes cantidades)
add_cost_calculator()

# Bot√≥n para ejecutar el clustering
if uploaded_file is not None and not st.session_state.process_complete:
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("Iniciar Clustering Sem√°ntico Avanzado", type="primary", use_container_width=True):
            success, results = run_clustering(
                uploaded_file, 
                openai_api_key, 
                num_clusters, 
                pca_variance, 
                max_pca_components, 
                min_df, 
                max_df,
                gpt_model
            )
            if success:
                st.session_state.df_results = results
                st.session_state.process_complete = True
                st.markdown("<div class='success-box'>‚úÖ Clustering sem√°ntico completado con √©xito!</div>", unsafe_allow_html=True)
# Mostrar resultados si el proceso est√° completo
if st.session_state.process_complete and st.session_state.df_results is not None:
    st.markdown("<div class='main-header'>Resultados del Clustering</div>", unsafe_allow_html=True)
    
    df = st.session_state.df_results
    
    # Pesta√±a para mostrar visualizaciones
    with st.expander("Visualizaciones", expanded=True):
        # Gr√°fico de barras con tama√±o de clusters
        st.subheader("Distribuci√≥n de Clusters")
        cluster_sizes = df.groupby(['cluster_id', 'cluster_name']).size().reset_index(name='count')
        cluster_sizes['label'] = cluster_sizes.apply(lambda x: f"{x['cluster_name']} (ID: {x['cluster_id']})", axis=1)
        
        fig = px.bar(
            cluster_sizes, 
            x='label', 
            y='count',
            color='count',
            labels={'count': 'N√∫mero de Keywords', 'label': 'Cluster'},
            title='Tama√±o de cada Cluster',
            color_continuous_scale=px.colors.sequential.Blues
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Gr√°fico de coherencia de clusters
        st.subheader("Coherencia Sem√°ntica de Clusters")
        
        coherence_data = df.groupby(['cluster_id', 'cluster_name'])['cluster_coherence'].mean().reset_index()
        coherence_data['label'] = coherence_data.apply(lambda x: f"{x['cluster_name']} (ID: {x['cluster_id']})", axis=1)
        
        fig2 = px.bar(
            coherence_data,
            x='label',
            y='cluster_coherence',
            color='cluster_coherence',
            labels={'cluster_coherence': 'Coherencia', 'label': 'Cluster'},
            title='Coherencia Sem√°ntica por Cluster',
            color_continuous_scale=px.colors.sequential.Greens
        )
        st.plotly_chart(fig2, use_container_width=True)
# Pesta√±a para explorar clusters
    with st.expander("Explorar Clusters", expanded=True):
        # Selector de cluster
        cluster_options = [f"{row['cluster_name']} (ID: {row['cluster_id']})" for _, row in 
                          df.drop_duplicates(['cluster_id', 'cluster_name'])[['cluster_id', 'cluster_name']].iterrows()]
        selected_cluster = st.selectbox("Selecciona un cluster para explorar:", cluster_options)
        
        if selected_cluster:
            # Obtener ID del cluster seleccionado
            cluster_id = int(selected_cluster.split("ID: ")[1].split(")")[0])
            
            # Filtrar datos del cluster seleccionado
            cluster_df = df[df['cluster_id'] == cluster_id].copy()
            
            # Mostrar informaci√≥n del cluster
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"### {cluster_df['cluster_name'].iloc[0]}")
                st.markdown(f"**Descripci√≥n:** {cluster_df['cluster_description'].iloc[0]}")
                st.markdown(f"**Total keywords:** {len(cluster_df)}")
            
            with col2:
                st.markdown(f"**Coherencia sem√°ntica:** {cluster_df['cluster_coherence'].iloc[0]:.3f}")
                st.markdown("**Keywords representativas:**")
                rep_keywords = cluster_df[cluster_df['representative'] == True]['keyword'].tolist()
                if rep_keywords:
                    st.markdown("<ul>" + "".join([f"<li>{kw}</li>" for kw in rep_keywords[:10]]) + "</ul>", unsafe_allow_html=True)
            
            # Mostrar todas las keywords del cluster
            st.markdown("### Todas las keywords")
            st.dataframe(cluster_df[['keyword']], use_container_width=True)

    # Pesta√±a para descargar resultados
    with st.expander("Descargar Resultados"):
        # Opci√≥n de descarga de CSV completo
        csv = df.to_csv(index=False)
        st.download_button(
            label="Descargar CSV con todos los resultados",
            data=csv,
            file_name="semantic_clustered_keywords.csv",
            mime="text/csv",
            use_container_width=True
        )
        
        # Opci√≥n de descarga de resumen
        st.subheader("Resumen de Clusters")
        summary_df = df.groupby(['cluster_id', 'cluster_name', 'cluster_description'])['keyword'].count().reset_index()
        summary_df.columns = ['ID', 'Nombre', 'Descripci√≥n', 'N√∫mero de Keywords']
        
        # A√±adir coherencia
        coherence_df = df.groupby('cluster_id')['cluster_coherence'].mean().reset_index()
        summary_df = summary_df.merge(coherence_df, left_on='ID', right_on='cluster_id')
        summary_df.drop('cluster_id', axis=1, inplace=True)
        summary_df.rename(columns={'cluster_coherence': 'Coherencia'}, inplace=True)
        
        # A√±adir keywords representativas
        def get_rep_keywords(cluster_id):
            reps = df[(df['cluster_id'] == cluster_id) & (df['representative'] == True)]['keyword'].tolist()
            return ', '.join(reps[:5])
        
        summary_df['Keywords Representativas'] = summary_df['ID'].apply(get_rep_keywords)
        
        st.dataframe(summary_df, use_container_width=True)
        
        # Descargar resumen
        csv_summary = summary_df.to_csv(index=False)
        st.download_button(
            label="Descargar resumen de clusters",
            data=csv_summary,
            file_name="semantic_clusters_summary.csv",
            mime="text/csv",
            use_container_width=True
        )
# Bot√≥n para reiniciar
if st.session_state.process_complete:
    if st.button("Reiniciar", type="secondary", use_container_width=True):
        st.session_state.process_complete = False
        st.session_state.df_results = None
        st.rerun()

# Informaci√≥n adicional
with st.expander("Informaci√≥n sobre el Clustering Sem√°ntico Avanzado"):
    st.markdown("""
    ### ¬øC√≥mo funciona este clustering sem√°ntico avanzado?
    
    1. **Preprocesamiento Ling√º√≠stico**: Las keywords se analizan usando NLP avanzado para extraer entidades nombradas, bigramas relevantes y tokens significativos.
    
    2. **Embeddings de Alta Calidad**: Se utilizan modelos de embeddings de √∫ltima generaci√≥n:
       - OpenAI Embeddings (hasta 5000 keywords) si se proporciona API key
       - Sentence Transformers (sin costo) como alternativa o fallback
       - TF-IDF como √∫ltimo recurso
    
    3. **Reducci√≥n Inteligente de Dimensionalidad**: PCA optimizado para preservar las relaciones sem√°nticas m√°s importantes.
    
    4. **Clustering Avanzado**: Algoritmos que descubren autom√°ticamente la estructura √≥ptima:
       - HDBSCAN para detectar clusters de forma natural
       - Clustering jer√°rquico aglomerativo optimizado
       - Determinaci√≥n autom√°tica del n√∫mero de clusters
    
    5. **Refinamiento Post-Clustering**: Identifica y corrige asignaciones problem√°ticas:
       - Detecci√≥n de outliers sem√°nticos
       - Fusi√≥n de clusters muy similares
       - Reasignaci√≥n de keywords mal clasificadas
    
    6. **Evaluaci√≥n Multi-M√©trica**: An√°lisis riguroso de la calidad de los clusters:
       - Coherencia sem√°ntica interna
       - Densidad y compacidad
       - Separaci√≥n entre clusters
       - Diagn√≥stico de clusters problem√°ticos
    
    ### Consejos para obtener mejores resultados
    
    - **Calidad de keywords**: El clustering funciona mejor cuando las keywords est√°n relacionadas con un mismo dominio o industria.
    
    - **Preprocesamiento**: Aseg√∫rate de que tus keywords no contengan errores ortogr√°ficos o caracteres extra√±os.
    
    - **API Key de OpenAI**: Proporciona una API Key para embeddings de mayor calidad, aunque SentenceTransformers ofrece buenos resultados sin costo.
    
    - **N√∫mero de clusters**: Considera usar la determinaci√≥n autom√°tica del n√∫mero √≥ptimo de clusters.
    
    - **Evaluaci√≥n iterativa**: Examina los clusters con baja coherencia y considera ajustar par√°metros o dividirlos.
    """)

# Footer
st.markdown("---")
st.markdown(
    """
    <div style="text-align: center; color: #888;">
        Desarrollado para clustering sem√°ntico avanzado de keywords | Versi√≥n 2.1 con OpenAI/SentenceTransformers h√≠brido
    </div>
    """, 
    unsafe_allow_html=True
)
